% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/evaluateRasters.R
\name{evaluateRasters}
\alias{evaluateRasters}
\title{Compare two rasters to evaluate prediction accuracy}
\usage{
evaluateRasters(obs, pred, metrics = "all", binary = FALSE,
  thresholds = NULL)
}
\arguments{
\item{obs}{a RasterLayer object, representing the Observed distribution used as a reference.}

\item{pred}{a RasterLayer object, representing the Predicted distribution from a model output.}

\item{metrics}{a vector containing a selection of following available metrics : \code{"RMSE"}, \code{"AUC"}, \code{"TSS"}, \code{"KAPPA"}, \code{"JACCARD"}, \code{"SORENSEN"} or \code{"all"} .
See details.}

\item{binary}{\code{TRUE} or \code{FALSE}. Should a binary transformation be used for suitability rasters.}

\item{thresholds}{a vector containing two numeric values. Will be used for binary tranformation of suitability rasters.}
}
\value{
\code{data.frame} with 2 columns, "Metric" and "Value"
}
\description{
This function compare two rasters using continuous and/or binary metrics.
If needed, a binary conversion using thresholds will be made to turn suitability rasters into presence/absence rasters.
}
\details{
This function can compute one continuous metric:
\itemize{
\item{\code{RMSE} : Root Main Squared Error (Caruana et Niculescu-Mizil, 2004).
The mean error between two suitability rasters.
}}

This function can also compute 5 binary metrics:
\itemize{
\item{\code{AUC} : Area Under the Receiver Operating Characteristic (Mason et Graham, 2002).
This metric compare a suitability prediction to a binary observation without the need of a threshold conversion.
}
\item{\code{TSS} : True Skill Statistic (Alouche et al., 2006). \code{TSS = specificity + sensitivity -1}
\itemize{
\item{\code{Sensitivity} (Fielding and Bell, 1997). Proportion of presences which are correctly identified as such.
}
\item{\code{Specificity} (Fielding and Bell, 1997). Proportion of absences which are correctly identified as such.
}
}}
\item{\code{KAPPA} : Cohen's Kappa (Cohen, 1960). The inter-rater agreement for qualitative items.
}
\item{\code{JACCARD} : Jaccard similarity coefficient (Jaccard, 1908). Similarity between finite sample sets.
\itemize{
\item{\code{OPR} : Over Prediction Rate (Barbosa et al., 2013). Proportion of false presences predicted.
}
\item{\code{UTP} : Unpredicted True Presences (Fielding and Bell, 1997). Proportion of true presences missed.
}
}}
\item{\code{SORENSEN} : Sorensen-Dice index, also known as F-measure (Daskallaki et al., 2006). Similarity between finite sample sets.
}

\item{\code{all} : Compute all of the above binary metrics.
}
}
}
\examples{
# Create an example observed suitability raster
a <- matrix(rep(dnorm(1:100, 40, sd = 25)),
            nrow = 100, ncol = 100, byrow = TRUE)
obs <- raster(a* dnorm(1:100, 50, sd = 25))

# Create an example of a bad predicted suitability raster
b1 <- matrix(rep(dnorm(1:100, 75, sd = 40)),
            nrow = 100, ncol = 100, byrow = TRUE)
pred1 <- raster(b1* dnorm(1:100, 50, sd = 25))

# Create an example of a good predicted suitability raster
b2 <- matrix(rep(dnorm(1:100, 45 , sd = 27)),
            nrow = 100, ncol = 100, byrow = TRUE)
pred2 <- raster(b2* dnorm(1:100, 50, sd = 25))

par(mfrow=c(1,3))
plot(obs, main="Reference raster")
plot(pred1, main='"Bad" prediction')
plot(pred2, main='"Good" prediction')

evaluateRasters(obs, pred1, binary=TRUE)
evaluateRasters(obs, pred2, binary=TRUE, metrics="all")
}
\author{
Robin Delsol \email{robin.delsol@wanadoo.fr}

Maintainer : Boris Leroy \email{leroy.boris@gmail.com}
}
\keyword{internal}

